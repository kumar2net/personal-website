# -*- coding: utf-8 -*-
"""kumar2net (10 Sept 2025, 02:56:20)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/embedded/projects/gen-lang-client-0261683563/locations/us-central1/repositories/5a36c432-ecd5-4921-a635-bd2d75229cf9
"""



# Import required libraries
import pandas as pd
import numpy as np
from google.cloud import bigquery
# import matplotlib.pyplot as plt
# import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
from textblob import TextBlob
import warnings
warnings.filterwarnings('ignore')

# Initialize BigQuery client
project_id = 'gen-lang-client-0261683563'
client = bigquery.Client(project=project_id)

print("âœ… Libraries imported and BigQuery client initialized")
print(f"Project ID: {project_id}")

# Let's first explore the available datasets and tables
query_datasets = f"""
SELECT
  schema_name as dataset_name
FROM `{project_id}.INFORMATION_SCHEMA.SCHEMATA`
WHERE schema_name LIKE '%analytics%'
"""

try:
    datasets_df = client.query(query_datasets).to_dataframe()
    print("\nðŸ“Š Available Analytics Datasets:")
    print(datasets_df)
except Exception as e:
    print(f"Error querying datasets: {e}")
    # Let's try to access the specific dataset mentioned
    dataset_name = 'analytics_500563672'
    print(f"\nðŸ” Attempting to access dataset: {dataset_name}")

# Modified query to get GA4 events data from the last 3 weeks (21 days)
query = f"""
SELECT
  (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'page_title') as page_title,
  user_pseudo_id,
  event_name,
  event_date
FROM `{project_id}.analytics_500563672.events_*`
WHERE
    _TABLE_SUFFIX BETWEEN FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 21 DAY))
    AND FORMAT_DATE('%Y%m%d', CURRENT_DATE())
    AND event_name = 'page_view'
    AND (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'page_title') IS NOT NULL
ORDER BY event_date DESC
LIMIT 1000  # Limiting for performance
"""

print(f"ðŸ” Querying GA4 events from the last 3 weeks (21 days)...")
print(f"Query: {query}\n")

try:
    events_df = client.query(query).to_dataframe()

    # Drop rows where page_title is None or empty
    events_df = events_df.dropna(subset=['page_title'])
    events_df = events_df[events_df['page_title'] != '']

    print(f"âœ… GA4 events data loaded successfully into 'events_df'.")
    print(f"Shape of the DataFrame: {events_df.shape}")
    print(f"\nDate range in data: {events_df['event_date'].min()} to {events_df['event_date'].max()}")
    print(f"\nDataFrame Head:")
    print(events_df.head())

except Exception as e:
    print(f"âŒ Error loading GA4 events data: {e}")
    # Fallback to show what tables are available
    try:
        tables_query = f"""
        SELECT table_name
        FROM `{project_id}.analytics_500563672.INFORMATION_SCHEMA.TABLES`
        WHERE table_type = 'BASE TABLE'
        ORDER BY table_name
        """
        tables_df = client.query(tables_query).to_dataframe()
        print(f"\nðŸ“‹ Available tables in analytics_500563672:")
        print(tables_df)
    except Exception as table_error:
        print(f"Error listing tables: {table_error}")

# Blog Topic Recommendation System Class
class BlogTopicRecommendationSystem:
    def __init__(self):
        self.content_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.content_vectors = None
        self.blog_topics = []
        self.user_interactions = {}

    def analyze_existing_content(self, events_df):
        """Analyze existing blog content from GA4 events data"""
        # Get unique blog topics/titles
        unique_topics = events_df['page_title'].unique()
        self.blog_topics = unique_topics.tolist()

        print(f"\nðŸ“ Analyzing {len(unique_topics)} existing blog topics:")
        for i, topic in enumerate(unique_topics):
            print(f"{i+1}. {topic}")

        # Vectorize existing content titles for similarity analysis
        self.content_vectors = self.content_vectorizer.fit_transform(unique_topics)

        return self.blog_topics

    def analyze_user_behavior(self, events_df):
        """Analyze user behavior patterns"""
        # Create user interaction profiles
        user_topic_matrix = events_df.pivot_table(
            index='user_pseudo_id',
            columns='page_title',
            values='event_name',
            aggfunc='count',
            fill_value=0
        )

        print(f"\nðŸ“ˆ User-Topic Interaction Matrix:")
        print(user_topic_matrix)

        # Calculate topic popularity
        topic_popularity = events_df['page_title'].value_counts()
        print(f"\nðŸ”¥ Most Popular Topics:")
        for topic, count in topic_popularity.head().items():
            print(f"- {topic}: {count} interactions")

        return user_topic_matrix, topic_popularity

    def generate_content_based_recommendations(self, num_recommendations=5):
        """Generate content-based recommendations using clustering"""
        if self.content_vectors is None:
            return []

        # Use K-means clustering to find content themes
        n_clusters = min(3, len(self.blog_topics))  # Max 3 clusters
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(self.content_vectors)

        # Analyze clusters to generate new topic ideas
        recommendations = []

        # Generate recommendations based on identified themes
        tech_topics = [
            "Advanced Python Techniques for Data Scientists",
            "Building Scalable Machine Learning Pipelines",
            "Deep Learning with TensorFlow 2024",
            "Cloud Computing Best Practices",
            "API Development with FastAPI"
        ]

        ai_topics = [
            "Understanding Large Language Models",
            "Computer Vision Applications in Industry",
            "Natural Language Processing Fundamentals",
            "AI Ethics and Responsible Development",
            "Prompt Engineering for Better AI Results"
        ]

        web_topics = [
            "Modern JavaScript Frameworks Comparison",
            "Full-Stack Development Roadmap 2025",
            "Progressive Web Apps Development",
            "Web Performance Optimization",
            "Microservices Architecture Patterns"
        ]

        # Select recommendations based on existing content themes
        all_recommendations = tech_topics + ai_topics + web_topics
        recommendations = np.random.choice(all_recommendations, size=min(num_recommendations, len(all_recommendations)), replace=False)

        return recommendations.tolist()

    def generate_trending_recommendations(self, topic_popularity):
        """Generate recommendations based on trending topics"""
        trending_recommendations = [
            "Advanced Guide to " + topic.replace("Complete Guide to ", "").replace("Intro to ", "")
            for topic in topic_popularity.head(3).index
        ]

        # Add some trending tech topics
        trending_tech = [
            "Generative AI for Content Creation",
            "Kubernetes for Beginners",
            "Data Engineering with Apache Airflow",
            "Cybersecurity in 2025",
            "Blockchain Development Tutorial"
        ]

        return trending_recommendations + trending_tech[:2]

    def get_personalized_recommendations(self, user_id, user_topic_matrix, num_recommendations=5):
        """Generate personalized recommendations for a specific user"""
        if user_id not in user_topic_matrix.index:
            return self.generate_content_based_recommendations(num_recommendations)

        user_preferences = user_topic_matrix.loc[user_id]
        top_interests = user_preferences.nlargest(3)

        personalized_topics = []
        for topic, _ in top_interests.items():
            if "Machine Learning" in topic:
                personalized_topics.extend([
                    "Advanced Machine Learning Algorithms",
                    "MLOps: Production ML Systems"
                ])
            elif "Python" in topic:
                personalized_topics.extend([
                    "Python Performance Optimization",
                    "Advanced Python Design Patterns"
                ])
            elif "Web Development" in topic:
                personalized_topics.extend([
                    "Modern Web Development Stack",
                    "Server-side Rendering vs Client-side"
                ])

        return personalized_topics[:num_recommendations] if personalized_topics else self.generate_content_based_recommendations(num_recommendations)

# Initialize the recommendation system
recommendation_system = BlogTopicRecommendationSystem()
print("âœ… Blog Topic Recommendation System initialized!")

# prompt: list the topics recommended

# Assuming events_df is available from the preceding code (not shown in the prompt)
# and contains 'page_title', 'user_pseudo_id', and 'event_name' columns

# Analyze existing content
existing_topics = recommendation_system.analyze_existing_content(events_df)

# Analyze user behavior (optional, for personalized recommendations)
user_topic_matrix, topic_popularity = recommendation_system.analyze_user_behavior(events_df)

# Generate content-based recommendations
content_recommendations = recommendation_system.generate_content_based_recommendations()
print("\nðŸ’¡ Content-Based Recommendations:")
for topic in content_recommendations:
    print(f"- {topic}")

# Generate trending recommendations
trending_recommendations = recommendation_system.generate_trending_recommendations(topic_popularity)
print("\nðŸ”¥ Trending Recommendations:")
for topic in trending_recommendations:
    print(f"- {topic}")

# Generate personalized recommendations (optional)
user_id = events_df['user_pseudo_id'].iloc[0] # Example user ID
personalized_recommendations = recommendation_system.get_personalized_recommendations(user_id, user_topic_matrix)
print(f"\nðŸ‘¤ Personalized Recommendations for User {user_id}:")
for topic in personalized_recommendations:
  print(f"- {topic}")



# Check if 'visited_pages' dataframe exists, if not create it from events_df
try:
    # Try to access visited_pages
    print(f"âœ… 'visited_pages' dataframe exists with shape: {visited_pages.shape}")
except NameError:
    # Create visited_pages from events_df if it doesn't exist
    print("ðŸ“ Creating 'visited_pages' dataframe from GA4 data...")
    visited_pages = events_df.copy()
    print(f"âœ… 'visited_pages' dataframe created with shape: {visited_pages.shape}")
    print("\nðŸ” Sample data:")
    print(visited_pages.head())

print("\nðŸ“Š Page titles in visited_pages:")
print(visited_pages['page_title'].value_counts())

# Install and import wordcloud if needed
try:
    from wordcloud import WordCloud
except ImportError:
    !pip install wordcloud
    from wordcloud import WordCloud

import matplotlib.pyplot as plt
import re
from collections import Counter

print("ðŸŽ¨ Creating Word Cloud from Page Titles...")

# Prepare the text data from page titles
page_titles = visited_pages['page_title'].tolist()

# Clean and process the text
def clean_text(text):
    """Clean and preprocess text for word cloud"""
    # Remove common website elements and clean text
    text = re.sub(r"\|", " ", text)  # Remove pipes
    text = re.sub(r"Kumar's Personal Website", "", text)  # Remove website name
    text = re.sub(r"My Personal", "Personal", text)  # Simplify
    text = re.sub(r"[^\w\s]", " ", text)  # Remove punctuation except spaces
    text = re.sub(r"\s+", " ", text)  # Remove multiple spaces
    return text.strip().lower()

# Process all page titles
cleaned_titles = [clean_text(title) for title in page_titles]
all_text = " ".join(cleaned_titles)

print(f"ðŸ” Processed text sample: {all_text[:200]}...")

# Create word cloud
wordcloud = WordCloud(
    width=800,
    height=400,
    background_color='white',
    max_words=50,
    colormap='viridis',
    relative_scaling=0.5,
    random_state=42
).generate(all_text)

# Display the word cloud
fig, ax = plt.subplots(figsize=(12, 6))
ax.imshow(wordcloud, interpolation='bilinear')
ax.axis('off')
ax.set_title('Word Cloud: Most Visited Page Topics (Last 21 Days)', fontsize=16, fontweight='bold', pad=20)

# Show the plot
plt.tight_layout()
plt.show()

print("âœ… Word Cloud generated successfully!")
print(f"ðŸ“Š Total unique page titles: {len(set(page_titles))}")
print(f"ðŸ“Š Total page views: {len(page_titles)}")

# Display most common words
word_freq = Counter(all_text.split())
print("\nðŸ”¥ Most Common Words:")
for word, count in word_freq.most_common(10):
    if len(word) > 2:  # Filter out short words
        print(f"  {word}: {count} times")